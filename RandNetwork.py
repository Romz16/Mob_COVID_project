# -*- coding: utf-8 -*-
"""IC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePK3fu588i2Q0TzVylzkb8Sac71tYYdP

#Instalando
"""

# ! sudo apt-get update
# ! sudo apt-get install texlive-latex-recommended
# ! sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended
# ! wget http://mirrors.ctan.org/macros/latex/...
# ! unzip type1cm.zip -d /tmp/type1cm
# ! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins
# ! sudo mkdir /usr/share/texmf/tex/latex/type1cm
# ! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm
# ! sudo texhash
# !apt install cm-super
# !pip install igraph

"""#Importando pacotes"""

import random
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from igraph import Graph
from scipy.optimize import curve_fit
from matplotlib import rc
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import csv
import random
import seaborn as sns



"""# Plotagem casos normalizados"""

def Acummulate (graph_name,list_cases_name,min_cases):
  graph = Graph.Read_GraphML(graph_name)

  geocodes = list(map(int, graph.vs["id"]))
  degrees = graph.degree()
  clustering = graph.transitivity_local_undirected()
  strength = graph.strength(weights="weight")
  graph.es['w_inv'] = 1.0 / np.array(graph.es['weight'])
  weighted_betweenness = graph.betweenness(vertices=None, directed=False, cutoff=None, weights='w_inv')
  weighted_closeness = graph.closeness(vertices=None, mode='all', cutoff=None, weights='w_inv', normalized=True)
  weighted_eignv = graph.evcent(directed=False, scale=True, weights='w_inv', return_eigenvalue=False)

  metrics_df = pd.DataFrame({
      "geocode": geocodes,
      "degree": degrees,
      "clustering": clustering,
      "strength": strength,
      "Weighted_betweenness": weighted_betweenness,
      "Weighted_closeness": weighted_closeness,
      "Weighted_eignv": weighted_eignv
  })
  degrees_avg = []
  betweenness_avg = []
  clustering_avg = []
  strength_avg = []
  closeness_avg = []
  eignv_avg = []
  def filter_cases(list_cases_name, min_cases):
      df = pd.read_csv(
          list_cases_name,
          encoding='utf-8',
          sep=',',
          usecols=['ibgeID', 'newCases', 'totalCases', 'date'],
          dtype={'ibgeID': int}
      )
      # print(df.shape)
      filtered_df = df[(df['totalCases'] >=  min_cases) & (df['newCases'] >= 1) & (df['ibgeID'] > 1000)]
      # print(filtered_df.shape)
      filtered_df = filtered_df.drop_duplicates(subset='ibgeID')
      return filtered_df

  df = filter_cases("/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv", min_cases[0])
  df_covid = df
  leng = []
  df = df.merge(metrics_df, left_on='ibgeID', right_on='geocode')
  df = df.sort_values("date")
  df_sum = df.groupby('date').sum().reset_index()
  leng.append(len(df['ibgeID']))

  df_sum['Degree Accumulated'] = df_sum['degree'].cumsum()
  df_sum['Clustering Accumulated'] = df_sum['clustering'].cumsum()
  df_sum['Strength Accumulated'] = df_sum['strength'].cumsum()
  df_sum['Betweenness Accumulated'] = df_sum['Weighted_betweenness'].cumsum()
  df_sum['Closeness Accumulated'] = df_sum['Weighted_closeness'].cumsum()
  df_sum['Eignv Accumulated'] = df_sum['Weighted_eignv'].cumsum()
  df_sum['Cases Accumulated'] = df_sum['newCases'].cumsum()
  metrics_table = df_sum[['date', 'Degree Accumulated', 'Clustering Accumulated', 'Strength Accumulated',
                              'Betweenness Accumulated', 'Closeness Accumulated', 'Eignv Accumulated', 'Cases Accumulated']]


  soma_degree = df['degree'].sum()
  soma_strength = df['strength'].sum()
  soma_clustering = df['clustering'].sum()
  soma_betweenness = df['Weighted_betweenness'].sum()
  soma_closeness = df['Weighted_closeness'].sum()
  soma_eignv = df['Weighted_eignv'].sum()


  dates= df['date'].unique()
  degree_interp = pd.Series(metrics_table['Degree Accumulated'] / soma_degree)
  clustering_interp =pd.Series( metrics_table['Clustering Accumulated'] / soma_clustering)
  strength_interp = pd.Series(metrics_table['Strength Accumulated'] / soma_strength)
  betweenness_interp = pd.Series(metrics_table['Betweenness Accumulated'] / soma_betweenness)
  closeness_interp = pd.Series(metrics_table['Closeness Accumulated'] / soma_closeness)
  eignv_interp = pd.Series(metrics_table['Eignv Accumulated'] / soma_eignv)


  df_covid['date'] = pd.to_datetime(df_covid['date'])
  df_covid = df_covid.sort_values('date')
  # print(df.shape)
  print(degree_interp)
  reference_column = 'newCases'
  df_date_cum_cases = df_covid.groupby('date')[reference_column].sum().reset_index()
  df_date_cum_cases[reference_column] = df_date_cum_cases[reference_column].cumsum()
  df_date_cum_cases.rename(columns={reference_column: 'Cumulative Cases per Day'}, inplace=True)
  df_date_cum_cases['Cumulative Cases per Day'] /= df_date_cum_cases['Cumulative Cases per Day'].max()

  df_date_cum_cases['Cases Accumulated'] = df_covid[reference_column].cumsum() / df_covid[reference_column].sum()
  # Remove duplicate dates, keeping only the first occurrence
  df_date_cum_cases = df_date_cum_cases.drop_duplicates('date', keep='first')

  def remove_non_matching_dates(df_date_cum_cases, dates):
      df_dates_str = df_date_cum_cases['date'].dt.strftime('%Y-%m-%d')
      set_dates = set(df_dates_str)
      set_dates_2 = set(dates)
      # Find dates that are in one set but not in the other
      dates_not_in_df = set_dates_2 - set_dates
      dates_not_in_dates = set_dates - set_dates_2
      # Combine the two lists of non-matching dates
      dates_not_matching = list(dates_not_in_df.union(dates_not_in_dates))
      # Remove rows from the DataFrame that correspond to the non-matching dates
      df_date_cum_cases_filtered = df_date_cum_cases[~df_dates_str.isin(dates_not_matching)]
      return df_date_cum_cases_filtered


  df_date_cum_cases=remove_non_matching_dates(df_date_cum_cases, dates)


  normalized_df = pd.DataFrame({
    'degree_interp': degree_interp.tolist(),
    'clustering_interp': clustering_interp.tolist(),
    'strength_interp': strength_interp.tolist(),
    'betweenness_interp': betweenness_interp.tolist(),
    'closeness_interp': closeness_interp.tolist(),
    'eignv_interp': eignv_interp.tolist()
    })
  return normalized_df,df_date_cum_cases




def SetComparison(graph_name,list_cases_name,min_cases):

  # Open the GraphML file and create a Graph object from it
  graph = Graph.Read_GraphML(graph_name)

  # Initialize lists to store average similarity values for each column
  degrees_integral = []
  betweenness_integral = []
  clustering_integral = []
  strength_integral = []
  closeness_w_integral = []
  eignv_w_integral = []
  Random_mean_integral = []
  Random_std_integral = []

  degrees_avg = []
  betweenness_avg = []
  clustering_avg = []
  strength_avg = []
  closeness_w_avg = []
  eignv_w_avg = []



  # Receive lists with filtered IDs
  covidID_list = filter_cases("/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv", min_cases[0]) #min_cases[0]
  metrics_list = filter_records(get_matrix(graph), covidID_list)
  covidID_list = filter_records2(metrics_list,covidID_list)
  metrics_matrix = np.array(metrics_list, dtype=float)
  covid_matrix = np.array(covidID_list)
  #Receive the size of the metrics matrix to match the size of the matrices
  matrix_size = len(metrics_matrix)
  #matrix_size = 100
  matrix_covid_dates = covid_matrix[:matrix_size, 0]
  Id_matrix_covid = covid_matrix[:matrix_size, 1].astype(float)

  # Calculate the similarity percentage for each metric
  degrees_similarity = compare_columns(metrics_matrix, 1,matrix_size,Id_matrix_covid)
  betweenness_similarity = compare_columns(metrics_matrix, 2,matrix_size,Id_matrix_covid)
  clustering_similarity = compare_columns(metrics_matrix, 3,matrix_size,Id_matrix_covid)
  strength_similarity = compare_columns(metrics_matrix, 4,matrix_size,Id_matrix_covid)
  closeness_w_similarity = compare_columns(metrics_matrix, 5,matrix_size,Id_matrix_covid)
  eignv_w_similarity = compare_columns(metrics_matrix, 6,matrix_size,Id_matrix_covid)

  data = {
    'degrees_similarity': degrees_similarity,
    'betweenness_similarity': betweenness_similarity,
    'clustering_similarity': clustering_similarity,
    'strength_similarity': strength_similarity,
    'closeness_w_similarity': closeness_w_similarity,
    'eignv_w_similarity': eignv_w_similarity
  }

  df = pd.DataFrame(data)
  return df,matrix_covid_dates

# Used to transform vertex data from 'graph' into a matrix
def get_matrix(graph):
    # Get vertex information
    # Calculate the inverse of edge weights and store them in the 'w_inv' property
    graph.es['w_inv'] = 1.0 / np.array(graph.es['weight'])
    vertex_info = []
    geocodes = graph.vs["id"]
    degrees = graph.degree()
    betweenness = graph.betweenness(vertices=None, directed=False, cutoff=None, weights='w_inv')
    clustering = graph.transitivity_local_undirected()
    strength = graph.strength(weights="weight")
    closeness_w = graph.closeness(vertices=None, mode='all', cutoff=None, weights='w_inv', normalized=True)
    eignv_w = graph.evcent(directed=False, scale=True, weights='w_inv', return_eigenvalue=False)
    #prank_w = nx.pagerank(g_nx, alpha=0.85, weight='weight')
    #vuln_w = vn.vulnerability(graph, weights='w_inv')
    geocodes_int = list(map(int, geocodes))
    geocodes_strings = list(map(str,geocodes_int))
    # Construct the vertex information matrix
    vertex_info = list(zip(geocodes_strings, degrees, betweenness, clustering, strength, closeness_w, eignv_w))

    return vertex_info

# Filter cases from a CSV file based on a minimum number of cases
# and return a list of filtered elements and their data
def filter_cases(csv_file, n):
    # Read the CSV file and define the columns to be considered
    df = pd.read_csv(
        csv_file,
        encoding='utf-8',
        sep=',',
        usecols=['ibgeID', 'newCases', 'totalCases', 'date'],
        dtype={'ibgeID': int}  # Define the appropriate data type for ibgeID if possible
    )
    # Filter records that meet the requirements
    filtered_df = df[(df['totalCases'] >= n) & (df['newCases'] >= 1) & (df['ibgeID'] != 0) & (df['ibgeID'] > 1000)]
    # Remove duplicate records based on ibgeID
    filtered_df = filtered_df.drop_duplicates(subset='ibgeID')
    # Return the results as a list of tuples
    filtered_cases = list(zip(filtered_df['date'], filtered_df['ibgeID'].apply(repr)))
    return filtered_cases



# Filter records from list A based on a filtered list of cities with more than N Covid cases
def filter_records(list_A, list_B):
    # Create sets of geocodes for easy verification
    geocode_set_A = set(record[0] for record in list_A)
    geocode_set_B = set(record[1] for record in list_B)
    # Find geocodes that exist in both sets
    common_geocodes = geocode_set_A.intersection(geocode_set_B)
    # Filter records that meet the criteria
    filtered_list = [record for record in list_A if record[0] in common_geocodes]
    return filtered_list

def filter_records2(list_A, list_B):
    # Create a set of all geocodes in list A for easy verification
    geocode_set_A = set(record[0] for record in list_A)
    # Filter records in list B that meet the criteria
    filtered_list = [record for record in list_B if record[1] in geocode_set_A]
    return filtered_list
# Verify the similarity between an ordered list of cities based on a certain metric
# and the list of cities with B Covid cases over time

def compare_random(metrics_matrix,matrix_size,Id_matrix_covid):
    # Select the columns from the matrices
    result = []
    for i in range(1, matrix_size + 1):
        col1_elements = set(Id_matrix_covid[:i])
        col2_elements = set(metrics_matrix[:i, 0].astype(float))
        intersection = col1_elements.intersection(col2_elements)
        similarity_percentage = (len(intersection) / len(col1_elements))
        result.append(similarity_percentage)
    return result

def compare_columns(metrics_matrix, col_idx,matrix_size,Id_matrix_covid):
    # Select the columns from the matrices
    result = []
    aux = metrics_matrix[metrics_matrix[:, col_idx].argsort()[::-1]]

    for i in range(1, matrix_size + 1):
        col1_elements = set(Id_matrix_covid[:i])

        col2_elements = set(aux[:i, 0].astype(float))
        intersection = col1_elements.intersection(col2_elements)
        similarity_percentage = (len(intersection) / len(col1_elements))

        result.append(similarity_percentage)
    return result

def calculate_random_metric_averages(data_matrix):

    Mselected_data = data_matrix.copy()
    # Extraia a segunda coluna (coluna 'geocode')
    # Embaralhe a lista de geocodes
    for _ in range(10):
        np.random.shuffle(Mselected_data)
    # Atualize a segunda coluna com os geocodes embaralhados
    similarity = compare_random(Mselected_data)
    result_data = {
        "Similarity": similarity}
    result_date_df = pd.DataFrame(result_data)
    result_date_df['id'] = range(1, len(result_date_df) + 1)
    return result_date_df

import os
import numpy as np
import polars as pl
import networkx as nx
import concurrent.futures




def construindo_rede_mobilidade(df_ibge, codigos_municipios):
    """
    Constrói a rede de mobilidade a partir dos dados do IBGE.

    Parâmetros:
    df_ibge (pl.DataFrame): DataFrame contendo os dados do IBGE.
    codigos_municipios (array): Array contendo os códigos dos municípios.

    Retorna:
    tuple: Contendo edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo.
    """
    # Dicionários para mapeamento de nomes e códigos dos municípios
    nome_municipios = {}
    codigo_para_indice = {}
    indice_para_codigo = {}

    # Preenchendo os dicionários com códigos e nomes dos municípios
    for i, codigo in enumerate(codigos_municipios):
        # Filtrando as linhas correspondentes ao código nas duas colunas
        linhas_a = df_ibge.filter(df_ibge['CODMUNDV_A'] == codigo)
        linhas_b = df_ibge.filter(df_ibge['CODMUNDV_B'] == codigo)

        # Selecionando o nome do município
        nome_municipio = linhas_a['NOMEMUN_A'][0] if not linhas_a.is_empty() else linhas_b['NOMEMUN_B'][0]

        # Adicionando ao dicionário
        nome_municipios[i] = nome_municipio
        codigo_para_indice[int(codigo)] = i
        indice_para_codigo[i] = int(codigo)

    # Criando o array de arestas (edges)
    edges = np.zeros((len(df_ibge), 2), dtype=int)

    # Preenchendo o array de arestas com os índices associados aos códigos únicos
    for i, (codigo_a, codigo_b) in enumerate(df_ibge[['CODMUNDV_A', 'CODMUNDV_B']].to_numpy()):
        edges[i, 0] = codigo_para_indice[codigo_a]
        edges[i, 1] = codigo_para_indice[codigo_b]

    # Calcula os pesos para cada aresta
    weights = (df_ibge['VAR05'] + df_ibge['VAR06'] + df_ibge['VAR12']).to_numpy()

    return edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo


def rede_aleatoria_processo_paralelismo(iteracao, edges, weights, indice_para_codigo):
    # Extrair e armazenar o vetor de graus da rede original
    grafo_original = nx.Graph()
    grafo_original.add_edges_from(edges)
    graus_originais = [grafo_original.degree(n) for n in grafo_original.nodes()]

    # Lista de pesos já extraída
    pesos_originais = weights

    # Lista de pares nome/geocódigo
    pares_nome_geocodigo = list(indice_para_codigo.items())

    # Embaralhar o vetor de graus
    graus_embaralhados = np.random.permutation(graus_originais)

    # Gerar a rede aleatorizada via Configuration Model
    rede_aleatorizada = nx.configuration_model(graus_embaralhados)
    rede_aleatorizada = nx.Graph(rede_aleatorizada)  # Remove self-loops e parallel edges

    # Atribuir pesos aleatórios a partir da lista de pesos originais
    pesos_aleatorios = np.random.choice(pesos_originais, size=rede_aleatorizada.number_of_edges())

    # Atribuir nomes/geocodes aos nós aleatoriamente
    np.random.shuffle(pares_nome_geocodigo)
    nome_geocode_dict = {id: geocode for id, geocode in pares_nome_geocodigo}

    # Atribuir os nomes e pesos aos nós e arestas
    mapping = {node: nome_geocode_dict[node] for node in rede_aleatorizada.nodes()}
    rede_aleatorizada = nx.relabel_nodes(rede_aleatorizada, mapping)

    # Adicionar atributo geocode aos nós
    for i, node in enumerate(rede_aleatorizada.nodes()):
        rede_aleatorizada.nodes[node]['geocode'] = nome_geocode_dict[i]

    # Atribuir pesos às arestas
    for j, (u, v) in enumerate(rede_aleatorizada.edges()):
        rede_aleatorizada.edges[u, v]['weight'] = pesos_aleatorios[j]

    # Salvar a rede aleatorizada em um arquivo GraphML
    graph_name = f'/media/work/romulorocha/IC/Datas/networks/rede_aleatorizada_{iteracao+1}.graphml'
    nx.write_graphml(rede_aleatorizada, graph_name)

    # Aqui você pode adicionar o código para rodar os Algoritmos 1 e 2 e coletar os resultados
    list_cases_name = "/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
    min_cases = [5]
    df_acc, Cumulative_Cases_Day = Acummulate(graph_name, list_cases_name, min_cases)
    df, matrix_covid_dates = SetComparison(graph_name, list_cases_name, min_cases)

    print(f"Iteração {iteracao + 1} completa.")
    
    return df, df_acc, matrix_covid_dates, Cumulative_Cases_Day

def gerar_rede_aleatorizada(edges, weights, indice_para_codigo, num_iteracoes=2, max_workers=None):
    # Armazenar os DataFrames em listas para posterior análise
    dataframes_stc_list = []
    dataframes_acc_list = []
    matrix_covid_dates_list = []
    cumulative_cases_day_list = []

    # Usar ThreadPoolExecutor para paralelizar as iterações
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(rede_aleatoria_processo_paralelismo, iteracao, edges, weights, indice_para_codigo) for iteracao in range(num_iteracoes)]
        
        for future in concurrent.futures.as_completed(futures):
            df, df_acc, matrix_covid_dates, cumulative_cases_day = future.result()
            dataframes_stc_list.append(df)
            dataframes_acc_list.append(df_acc)
            matrix_covid_dates_list.append(matrix_covid_dates)
            cumulative_cases_day_list.append(cumulative_cases_day)

    print(dataframes_acc_list)
    # Retorne os resultados desejados
    return dataframes_stc_list, dataframes_acc_list, matrix_covid_dates_list, cumulative_cases_day_list


# Define o caminho para o arquivo do DataFrame do IBGE de 2016 na pasta 'raw_data'
caminho_df_ibge = '/media/work/romulorocha/IC/Datas/Pre-processed/dataset_transform_IBGE.xlsx'

# Carregando o dataset IBGE de ligações entre cidades com pesos
df_ibge = pl.read_excel(caminho_df_ibge)

# Obtendo códigos únicos de ambas as colunas
codigos_municipios = np.unique(
    np.concatenate([df_ibge['CODMUNDV_A'].to_numpy(), df_ibge['CODMUNDV_B'].to_numpy()]))

# Construindo a rede de mobilidade
edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo = (
    construindo_rede_mobilidade(df_ibge, codigos_municipios))


# Chamando a função para gerar a rede aleatorizada
dataframes_stc_list, dataframes_acc_list, matrix_covid_dates_list, cumulative_cases_day_list = gerar_rede_aleatorizada(edges, weights, indice_para_codigo, num_iteracoes=700, max_workers=20)

# Calcular a média de casos aleatorios
concatenated_df = pd.concat(dataframes_stc_list, axis=0)
mean_df_stc = concatenated_df.groupby(concatenated_df.index).mean()

concatenated_df = pd.concat(dataframes_acc_list, axis=0)
mean_df_acc = concatenated_df.groupby(concatenated_df.index).mean()

list_cases_name = "/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
min_cases = [5]
leng = len(mean_df_stc)
step = 255

file_path_stc = '/media/work/romulorocha/IC/mean_df_stc.csv'
file_path_acc = '/media/work/romulorocha/IC/mean_df_acc.csv'

# Salvar mean_df_stc em um arquivo CSV
mean_df_stc.to_csv(file_path_stc, index=False)
print(f'mean_df_stc salvo no arquivo {file_path_stc}')

# Salvar mean_df_acc em um arquivo CS
mean_df_acc.to_csv(file_path_acc, index=False)
print(f'mean_df_acc salvo no arquivo {file_path_acc}')



# Salvar cumulative_cases_day_list em um arquivo CSV
file_path_cumulative_cases_day = '/media/work/romulorocha/IC/cumulative_cases_day_list.csv'
concatenated_cumulative_cases_day = pd.concat(cumulative_cases_day_list, axis=0)
concatenated_cumulative_cases_day.to_csv(file_path_cumulative_cases_day, index=False)
print(f'cumulative_cases_day_list salvo no arquivo {file_path_cumulative_cases_day}')
