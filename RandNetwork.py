# -*- coding: utf-8 -*-
"""IC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePK3fu588i2Q0TzVylzkb8Sac71tYYdP

#Instalando
"""

# ! sudo apt-get update
# ! sudo apt-get install texlive-latex-recommended
# ! sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended
# ! wget http://mirrors.ctan.org/macros/latex/...
# ! unzip type1cm.zip -d /tmp/type1cm
# ! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins
# ! sudo mkdir /usr/share/texmf/tex/latex/type1cm
# ! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm
# ! sudo texhash
# !apt install cm-super
# !pip install igraph

"""#Importando pacotes"""

import random
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from igraph import Graph
from scipy.optimize import curve_fit
from matplotlib import rc
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import csv
import random
import seaborn as sns

"""#Setando parametro de plotagem"""

rc('text', usetex=True)
font = {'family' : 'normal',
         'weight' : 'bold',
         'size'   : 12}

rc('font', **font)
params = {'legend.fontsize': 12}
plt.rcParams.update(params)
plt.rcParams.update({
    "text.usetex": True,
    "font.family": "sans-serif",
    "font.sans-serif":"Helvetica",
})

# from google.colab import drive
# drive.mount('/content/drive')

"""#Plot AUC x Numero de casos"""


def logistic_growth(x, a, b, k):
    return a / (1 + b * np.exp(-k * x))

rc('text', usetex=True)
font = {'family' : 'normal',
         'weight' : 'bold',
         'size'   : 12}

rc('font', **font)
params = {'legend.fontsize': 12}
plt.rcParams.update(params)
plt.rcParams.update({
    "text.usetex": True,
    "font.family": "sans-serif",
    "font.sans-serif":"Helvetica",
})

def plotGraph(leng, minimum_cases):
    upper_bound = mean_table + 2 * std_table
    lower_bound = mean_table - 2 * std_table

    point_indices = np.linspace(0, len(dates) - 1, num=30)
    date_indices = np.arange(len(dates))
    soma_degree = df['degree'].sum()
    soma_strength = df['strength'].sum()
    soma_clustering = df['clustering'].sum()
    soma_betweenness = df['Weighted_betweenness'].sum()
    soma_closeness = df['Weighted_closeness'].sum()
    soma_eignv = df['Weighted_eignv'].sum()
    soma_cases = metrics_table['Cases Accumulated'].sum()
    degree_interp = np.interp(point_indices, date_indices, metrics_table['Degree Accumulated']) / soma_degree
    clustering_interp = np.interp(point_indices, date_indices, metrics_table['Clustering Accumulated']) / soma_clustering
    strength_interp = np.interp(point_indices, date_indices, metrics_table['Strength Accumulated']) / soma_strength
    betweenness_interp = np.interp(point_indices, date_indices, metrics_table['Betweenness Accumulated']) / soma_betweenness
    closeness_interp = np.interp(point_indices, date_indices, metrics_table['Closeness Accumulated']) / soma_closeness
    eignv_interp = np.interp(point_indices, date_indices, metrics_table['Eignv Accumulated']) / soma_eignv
    cases_interp = np.interp(point_indices, date_indices, metrics_table['Cases Accumulated']) / soma_cases
    # Store the interpolation data in a pandas DataFrame
    interp_data_df = pd.DataFrame({
        'Degree': degree_interp,
        'Weighted Betweenness': betweenness_interp,
        'Clustering': clustering_interp,
        'Strength': strength_interp,
        'Weighted Closeness': closeness_interp,
        'Weighted Eigenvector': eignv_interp,
        'Cumulative Cases per Day': cases_interp,
    })

    # Calculate the area under each curve using numerical integration (trapezoidal rule)
    areas = interp_data_df.apply(lambda col: np.trapz(col, dx=1))

    popt_clustering, _ = curve_fit(logistic_growth, point_indices, clustering_interp)
    popt_betweenness, _ = curve_fit(logistic_growth, point_indices, betweenness_interp)

    clustering_fit = logistic_growth(point_indices, *popt_clustering)
    betweenness_fit = logistic_growth(point_indices, *popt_betweenness)




    plt.figure(figsize=(12, 6))
    plt.plot(point_indices, degree_interp, label=f'Degree', marker='$k$',color ='SlateBlue')
    plt.plot(point_indices, betweenness_interp, label=f'Weighted Betweenness', marker='$s$',color ='DarkSlateBlue')
    plt.plot(point_indices, clustering_interp, label=f'Clustering', marker='$b$',color ='RebeccaPurple')
    plt.plot(point_indices, strength_interp, label=f'Strength', marker='$b_w$',color ='MediumOrchid')
    plt.plot(point_indices, closeness_interp, label=f'Weighted Closeness', marker='$c$',color ='SteelBlue')
    plt.plot(point_indices, eignv_interp, label=f'Weighted Eigenvector', marker='$c_w$',color ='DarkTurquoise')
    plt.plot(point_indices,cases_interp, marker='o', linestyle='-', color='b', label='Normalized Cumulative Cases')
    # plt.plot(point_indices, cases_interp, marker='$t_c$', color='orange', label='Total Cases')

    #plt.plot(point_indices, cases_interp, marker='o', linestyle='-', color='b', label='Normalized Accumulated New Cases')

    plt.fill_between(mean_table.index, lower_bound['Degree'], upper_bound['Degree'], color='gray', alpha=0.3)
    plt.fill_between(mean_table.index, lower_bound['Clustering'], upper_bound['Clustering'], color='gray', alpha=0.3)
    plt.fill_between(mean_table.index, lower_bound['Strength'], upper_bound['Strength'], color='gray', alpha=0.3)
    plt.fill_between([], [], [], color='gray', alpha=0.3, label='Development with Random Cases')

    plt.plot(point_indices, clustering_fit, 'g--')
    plt.plot(point_indices, betweenness_fit, 'b--')



    plt.annotate(f'Logistic Fit (Clustering)\nA: {popt_clustering[0]:.2f}, B: {popt_clustering[1]:.2f}, K: {popt_clustering[2]:.2f}',
                 xy=(point_indices[5], clustering_fit[5]), xycoords='data',
                 xytext=(point_indices[8], clustering_fit[6]), textcoords='data',
                 arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))

    plt.annotate(f'Logistic Fit (Weighted Betweenness)\nA: {popt_betweenness[0]:.2f}, B: {popt_betweenness[1]:.2f}, K: {popt_betweenness[2]:.2f}',
                 xy=(point_indices[4], betweenness_fit[4]), xycoords='data',
                 xytext=(point_indices[0]+ 0.3, betweenness_fit[8]), textcoords='data',
                 arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))



    plt.legend(fontsize='medium')
    plt.text(0.8, 0.90, f'Number of Cities: {leng}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)

    visible_indices = np.arange(0, len(metrics_table), step=30)
    visible_dates = metrics_table['date'].iloc[visible_indices]
    plt.xticks(visible_indices, visible_dates, rotation=45)

    plt.xlabel('Date')
    plt.ylabel(f'Normalized Cumulative Cases per Day')
    plt.tight_layout()
    plt.xlim(0, len(metrics_table) - 1)
    plt.ylim(0, 1)
    plt.savefig(f'/media/work/romulorocha/IC/Accumulated_metricas_{minimum_cases}cases_RANDNET.pdf', bbox_inches='tight')
    plt.show()
    plt.close()

#print(metrics_table)



#metrics_df

#df_sum

#metrics_table

# metrics_table = metrics_table.reset_index(drop=True)
# dates = df_sum['date']
# result_list = [calculate_random_metric_averages(df) for _ in range(500)]
# combined_table = pd.concat(result_list)
# mean_table = combined_table.groupby('Date').mean()
# std_table = combined_table.groupby('Date').std()

# plotGraph(leng[0],min_cases[0])

#df_sum.describe()

"""# Plotagem casos normalizados"""

def Acummulate (graph_name,list_cases_name,min_cases):
  graph = Graph.Read_GraphML(graph_name)

  geocodes = list(map(int, graph.vs["id"]))
  degrees = graph.degree()
  clustering = graph.transitivity_local_undirected()
  strength = graph.strength(weights="weight")
  graph.es['w_inv'] = 1.0 / np.array(graph.es['weight'])
  weighted_betweenness = graph.betweenness(vertices=None, directed=False, cutoff=None, weights='w_inv')
  weighted_closeness = graph.closeness(vertices=None, mode='all', cutoff=None, weights='w_inv', normalized=True)
  weighted_eignv = graph.evcent(directed=False, scale=True, weights='w_inv', return_eigenvalue=False)

  metrics_df = pd.DataFrame({
      "geocode": geocodes,
      "degree": degrees,
      "clustering": clustering,
      "strength": strength,
      "Weighted_betweenness": weighted_betweenness,
      "Weighted_closeness": weighted_closeness,
      "Weighted_eignv": weighted_eignv
  })
  degrees_avg = []
  betweenness_avg = []
  clustering_avg = []
  strength_avg = []
  closeness_avg = []
  eignv_avg = []
  def filter_cases(list_cases_name, min_cases):
      df = pd.read_csv(
          list_cases_name,
          encoding='utf-8',
          sep=',',
          usecols=['ibgeID', 'newCases', 'totalCases', 'date'],
          dtype={'ibgeID': int}
      )
      # print(df.shape)
      filtered_df = df[(df['totalCases'] >=  min_cases) & (df['newCases'] >= 1) & (df['ibgeID'] > 1000)]
      # print(filtered_df.shape)
      filtered_df = filtered_df.drop_duplicates(subset='ibgeID')
      return filtered_df

  df = filter_cases("/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv", min_cases[0])
  df_covid = df
  leng = []
  df = df.merge(metrics_df, left_on='ibgeID', right_on='geocode')
  df = df.sort_values("date")
  df_sum = df.groupby('date').sum().reset_index()
  leng.append(len(df['ibgeID']))

  df_sum['Degree Accumulated'] = df_sum['degree'].cumsum()
  df_sum['Clustering Accumulated'] = df_sum['clustering'].cumsum()
  df_sum['Strength Accumulated'] = df_sum['strength'].cumsum()
  df_sum['Betweenness Accumulated'] = df_sum['Weighted_betweenness'].cumsum()
  df_sum['Closeness Accumulated'] = df_sum['Weighted_closeness'].cumsum()
  df_sum['Eignv Accumulated'] = df_sum['Weighted_eignv'].cumsum()
  df_sum['Cases Accumulated'] = df_sum['newCases'].cumsum()
  metrics_table = df_sum[['date', 'Degree Accumulated', 'Clustering Accumulated', 'Strength Accumulated',
                              'Betweenness Accumulated', 'Closeness Accumulated', 'Eignv Accumulated', 'Cases Accumulated']]


  soma_degree = df['degree'].sum()
  soma_strength = df['strength'].sum()
  soma_clustering = df['clustering'].sum()
  soma_betweenness = df['Weighted_betweenness'].sum()
  soma_closeness = df['Weighted_closeness'].sum()
  soma_eignv = df['Weighted_eignv'].sum()


  dates= df['date'].unique()
  degree_interp = pd.Series(metrics_table['Degree Accumulated'] / soma_degree)
  clustering_interp =pd.Series( metrics_table['Clustering Accumulated'] / soma_clustering)
  strength_interp = pd.Series(metrics_table['Strength Accumulated'] / soma_strength)
  betweenness_interp = pd.Series(metrics_table['Betweenness Accumulated'] / soma_betweenness)
  closeness_interp = pd.Series(metrics_table['Closeness Accumulated'] / soma_closeness)
  eignv_interp = pd.Series(metrics_table['Eignv Accumulated'] / soma_eignv)


  df_covid['date'] = pd.to_datetime(df_covid['date'])
  df_covid = df_covid.sort_values('date')
  # print(df.shape)
  print(degree_interp)
  reference_column = 'newCases'
  df_date_cum_cases = df_covid.groupby('date')[reference_column].sum().reset_index()
  df_date_cum_cases[reference_column] = df_date_cum_cases[reference_column].cumsum()
  df_date_cum_cases.rename(columns={reference_column: 'Cumulative Cases per Day'}, inplace=True)
  df_date_cum_cases['Cumulative Cases per Day'] /= df_date_cum_cases['Cumulative Cases per Day'].max()

  df_date_cum_cases['Cases Accumulated'] = df_covid[reference_column].cumsum() / df_covid[reference_column].sum()
  # Remove duplicate dates, keeping only the first occurrence
  df_date_cum_cases = df_date_cum_cases.drop_duplicates('date', keep='first')

  def remove_non_matching_dates(df_date_cum_cases, dates):
      df_dates_str = df_date_cum_cases['date'].dt.strftime('%Y-%m-%d')
      set_dates = set(df_dates_str)
      set_dates_2 = set(dates)
      # Find dates that are in one set but not in the other
      dates_not_in_df = set_dates_2 - set_dates
      dates_not_in_dates = set_dates - set_dates_2
      # Combine the two lists of non-matching dates
      dates_not_matching = list(dates_not_in_df.union(dates_not_in_dates))
      # Remove rows from the DataFrame that correspond to the non-matching dates
      df_date_cum_cases_filtered = df_date_cum_cases[~df_dates_str.isin(dates_not_matching)]
      return df_date_cum_cases_filtered


  df_date_cum_cases=remove_non_matching_dates(df_date_cum_cases, dates)


  normalized_df = pd.DataFrame({
    'degree_interp': degree_interp.tolist(),
    'clustering_interp': clustering_interp.tolist(),
    'strength_interp': strength_interp.tolist(),
    'betweenness_interp': betweenness_interp.tolist(),
    'closeness_interp': closeness_interp.tolist(),
    'eignv_interp': eignv_interp.tolist()
    })
  return normalized_df,df_date_cum_cases


def plot_acc_normalized(normalized_df,leng,df_date_cum_cases):

  degree_interp = normalized_df['degree_interp'].values
  clustering_interp = normalized_df['clustering_interp'].values
  strength_interp = normalized_df['strength_interp'].values
  betweenness_interp = normalized_df['betweenness_interp'].values
  closeness_interp = normalized_df['closeness_interp'].values
  eignv_interp = normalized_df['eignv_interp'].values

  # upper_bound = mean_table + 2 * std_table
  # lower_bound = mean_table - 2 * std_table

  visible_points_quantity = 30
  sampled_indices = np.linspace(0, len(df_date_cum_cases) - 1, visible_points_quantity, dtype=int)
  df_date_cum_cases_amostrado = df_date_cum_cases.iloc[sampled_indices]

  degree_interp_amostrado = degree_interp[sampled_indices]
  betweenness_interp_amostrado = betweenness_interp[sampled_indices]
  clustering_interp_amostrado = clustering_interp[sampled_indices]
  strength_interp_amostrado = strength_interp[sampled_indices]
  closeness_interp_amostrado = closeness_interp[sampled_indices]
  eignv_interp_amostrado = eignv_interp[sampled_indices]
  def logistic_growth(x, a, b, k):
    return a / (1 + b * np.exp(-k * x))

  first_day_amostrado = df_date_cum_cases_amostrado['date'].min()
  dates_array= (df_date_cum_cases_amostrado['date'] - first_day_amostrado).dt.days.values
  popt_clustering, _ = curve_fit(logistic_growth,dates_array, clustering_interp_amostrado)
  popt_betweenness, _ = curve_fit(logistic_growth, dates_array, betweenness_interp_amostrado)


  clustering_fit = logistic_growth(dates_array, *popt_clustering)
  betweenness_fit = logistic_growth(dates_array, *popt_betweenness)

  plt.figure(figsize=(14, 6))
  plt.plot(df_date_cum_cases_amostrado['date'], df_date_cum_cases_amostrado['Cumulative Cases per Day'],'--', linestyle='-', color='r', label='Normalized Cumulative Cases')
  plt.plot(df_date_cum_cases_amostrado['date'], degree_interp_amostrado, label=f'Degree', marker='$o$', color='SlateBlue')
  plt.plot(df_date_cum_cases_amostrado['date'], betweenness_interp_amostrado, label=f'Weighted Betweenness', marker='$x$', color='DarkSlateBlue')
  plt.plot(df_date_cum_cases_amostrado['date'], clustering_interp_amostrado, label=f'Clustering',  marker='$s$', color='RebeccaPurple')
  plt.plot(df_date_cum_cases_amostrado['date'], strength_interp_amostrado, label=f'Strength', marker='^', color='MediumOrchid')
  plt.plot(df_date_cum_cases_amostrado['date'], closeness_interp_amostrado, label=f'Weighted Closeness',  marker='>', color='SteelBlue')
  plt.plot(df_date_cum_cases_amostrado['date'], eignv_interp_amostrado, label=f'Weighted Eigenvector', marker='+', color='DarkTurquoise')

  # plt.fill_between(mean_table.index, lower_bound['Clustering'], upper_bound['Clustering'], color='gray', alpha=0.3)
  # plt.fill_between(mean_table.index, lower_bound['Strength'], upper_bound['Strength'], color='gray', alpha=0.3)
  # plt.fill_between(mean_table.index, lower_bound['Degree'], upper_bound['Degree'], color='gray', alpha=0.3)

  plt.plot(df_date_cum_cases_amostrado['date'], clustering_fit, 'g--')
  plt.plot(df_date_cum_cases_amostrado['date'], betweenness_fit, 'b--')

  from datetime import timedelta
  plt.annotate(f'Logistic Fit (Clustering)\nA: {popt_clustering[0]:.2f}, B: {popt_clustering[1]:.2f}, K: {popt_clustering[2]:.2f}',
                  xy=(df_date_cum_cases_amostrado['date'].iloc[5], clustering_fit[5]), xycoords='data',
                  xytext=(df_date_cum_cases_amostrado['date'].iloc[8], clustering_fit[6]), textcoords='data',
                  arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))

  plt.annotate(f'Logistic Fit (Weighted Betweenness)\nA: {popt_betweenness[0]:.2f}, B: {popt_betweenness[1]:.2f}, K: {popt_betweenness[2]:.2f}',
                  xy=(df_date_cum_cases_amostrado['date'].iloc[4], betweenness_fit[4]), xycoords='data',
                  xytext=(df_date_cum_cases_amostrado['date'].iloc[0]+ timedelta(days=0.3), betweenness_fit[8]), textcoords='data',
                  arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))


 # plt.fill_between([], [], [], color='gray', alpha=0.3, label='Development with Random Cases')

  #plt.plot(df_date_cum_cases['date'], regression_predictions, color='r', linestyle='--', label='Linear Regression (Cumulative Cases)')
  l = leng
  plt.legend(fontsize='medium')
  plt.text(0.8, 0.90, f'Number of Cities: {l}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)

  plt.xlabel('Date')
  plt.ylabel('Normalized Cumulative Cases per Day')
  plt.xticks(rotation=45)
  # plt.xlim(df['date'][0], df['date'][-1])
  plt.autoscale(axis='x', tight=True)
  plt.autoscale(axis='y', tight=True)
  plt.ylim(0,1)
  plt.legend()
  plt.tight_layout()
  plt.savefig(f'/media/work/romulorocha/IC/Accumulated_metricas_RANDNET_cases.pdf', bbox_inches='tight')
  plt.show()

# graph_name = "IC/Datas/networks/rede_aleatorizada.GraphML"
# list_cases_name = "IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
# min_cases = 5
# df = Acummulate(graph_name,list_cases_name,min_cases)



"""# Pearson Correlation"""

# df_pearson = pd.DataFrame({
#     # 'date': df_date_cum_cases_amostrado['date'],
#     'Normalized\nCumulative\n Cases per Day': df_date_cum_cases['Cumulative Cases per Day'],
#     'Betweenness': betweenness_interp,
#     'Degree': degree_interp,
#     'Closeness': closeness_interp,
#     'Clustering': clustering_interp,
#     'Eigenvector': eignv_interp,
#     'Strength': strength_interp,
# })

# corr = df_pearson.corr()

# mask = np.triu(np.ones_like(corr, dtype=bool))

# plt.figure(figsize=(12, 8))
# sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# plt.xticks(rotation=0)
# plt.savefig(f'IC/Correlation Pearson.pdf', bbox_inches='tight')
# plt.show()

# df_kendall = df_pearson.corr(method='kendall')
# df_spearman = df_pearson.corr(method='spearman')

# mask = np.triu(np.ones_like(df_kendall, dtype=bool))

# plt.figure(figsize=(12, 8))
# sns.heatmap(df_kendall, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# plt.savefig(f'IC/Correlation Kendall.pdf', bbox_inches='tight')
# plt.show()

# plt.figure(figsize=(12, 8))
# sns.heatmap(df_spearman, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# plt.savefig(f'IC/Correlation Spearman.pdf', bbox_inches='tight')
# plt.show()

"""# Set comparison

```
# Isto está formatado como código
```


"""

def SetComparison(graph_name,list_cases_name,min_cases):

  # Open the GraphML file and create a Graph object from it
  graph = Graph.Read_GraphML(graph_name)

  # Initialize lists to store average similarity values for each column
  degrees_integral = []
  betweenness_integral = []
  clustering_integral = []
  strength_integral = []
  closeness_w_integral = []
  eignv_w_integral = []
  Random_mean_integral = []
  Random_std_integral = []

  degrees_avg = []
  betweenness_avg = []
  clustering_avg = []
  strength_avg = []
  closeness_w_avg = []
  eignv_w_avg = []



  # Receive lists with filtered IDs
  covidID_list = filter_cases("/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv", min_cases[0]) #min_cases[0]
  metrics_list = filter_records(get_matrix(graph), covidID_list)
  covidID_list = filter_records2(metrics_list,covidID_list)
  metrics_matrix = np.array(metrics_list, dtype=float)
  covid_matrix = np.array(covidID_list)
  #Receive the size of the metrics matrix to match the size of the matrices
  matrix_size = len(metrics_matrix)
  #matrix_size = 100
  matrix_covid_dates = covid_matrix[:matrix_size, 0]
  Id_matrix_covid = covid_matrix[:matrix_size, 1].astype(float)
  # result_list = [calculate_random_metric_averages(metrics_matrix) for _ in range(300)]
  # combined_table = pd.concat(result_list)
  # mean_table = combined_table.groupby('id').mean()
  # std_table = combined_table.groupby('id').std()
  # upper_bound = mean_table + 2 * std_table
  # lower_bound = mean_table - 2 * std_table
  # setUp_comparsion_table(min_cases[0])
  #print(degrees_similarity)


  # Calculate the similarity percentage for each metric
  degrees_similarity = compare_columns(metrics_matrix, 1,matrix_size,Id_matrix_covid)
  betweenness_similarity = compare_columns(metrics_matrix, 2,matrix_size,Id_matrix_covid)
  clustering_similarity = compare_columns(metrics_matrix, 3,matrix_size,Id_matrix_covid)
  strength_similarity = compare_columns(metrics_matrix, 4,matrix_size,Id_matrix_covid)
  closeness_w_similarity = compare_columns(metrics_matrix, 5,matrix_size,Id_matrix_covid)
  eignv_w_similarity = compare_columns(metrics_matrix, 6,matrix_size,Id_matrix_covid)

  data = {
    'degrees_similarity': degrees_similarity,
    'betweenness_similarity': betweenness_similarity,
    'clustering_similarity': clustering_similarity,
    'strength_similarity': strength_similarity,
    'closeness_w_similarity': closeness_w_similarity,
    'eignv_w_similarity': eignv_w_similarity
  }

  df = pd.DataFrame(data)
  return df,matrix_covid_dates

# Used to transform vertex data from 'graph' into a matrix
def get_matrix(graph):
    # Get vertex information
    # Calculate the inverse of edge weights and store them in the 'w_inv' property
    graph.es['w_inv'] = 1.0 / np.array(graph.es['weight'])
    vertex_info = []
    geocodes = graph.vs["id"]
    degrees = graph.degree()
    betweenness = graph.betweenness(vertices=None, directed=False, cutoff=None, weights='w_inv')
    clustering = graph.transitivity_local_undirected()
    strength = graph.strength(weights="weight")
    closeness_w = graph.closeness(vertices=None, mode='all', cutoff=None, weights='w_inv', normalized=True)
    eignv_w = graph.evcent(directed=False, scale=True, weights='w_inv', return_eigenvalue=False)
    #prank_w = nx.pagerank(g_nx, alpha=0.85, weight='weight')
    #vuln_w = vn.vulnerability(graph, weights='w_inv')
    geocodes_int = list(map(int, geocodes))
    geocodes_strings = list(map(str,geocodes_int))
    # Construct the vertex information matrix
    vertex_info = list(zip(geocodes_strings, degrees, betweenness, clustering, strength, closeness_w, eignv_w))

    return vertex_info

# Filter cases from a CSV file based on a minimum number of cases
# and return a list of filtered elements and their data
def filter_cases(csv_file, n):
    # Read the CSV file and define the columns to be considered
    df = pd.read_csv(
        csv_file,
        encoding='utf-8',
        sep=',',
        usecols=['ibgeID', 'newCases', 'totalCases', 'date'],
        dtype={'ibgeID': int}  # Define the appropriate data type for ibgeID if possible
    )
    # Filter records that meet the requirements
    filtered_df = df[(df['totalCases'] >= n) & (df['newCases'] >= 1) & (df['ibgeID'] != 0) & (df['ibgeID'] > 1000)]
    # Remove duplicate records based on ibgeID
    filtered_df = filtered_df.drop_duplicates(subset='ibgeID')
    # Return the results as a list of tuples
    filtered_cases = list(zip(filtered_df['date'], filtered_df['ibgeID'].apply(repr)))
    return filtered_cases



# Filter records from list A based on a filtered list of cities with more than N Covid cases
def filter_records(list_A, list_B):
    # Create sets of geocodes for easy verification
    geocode_set_A = set(record[0] for record in list_A)
    geocode_set_B = set(record[1] for record in list_B)
    # Find geocodes that exist in both sets
    common_geocodes = geocode_set_A.intersection(geocode_set_B)
    # Filter records that meet the criteria
    filtered_list = [record for record in list_A if record[0] in common_geocodes]
    return filtered_list

def filter_records2(list_A, list_B):
    # Create a set of all geocodes in list A for easy verification
    geocode_set_A = set(record[0] for record in list_A)
    # Filter records in list B that meet the criteria
    filtered_list = [record for record in list_B if record[1] in geocode_set_A]
    return filtered_list
# Verify the similarity between an ordered list of cities based on a certain metric
# and the list of cities with B Covid cases over time

def compare_random(metrics_matrix,matrix_size,Id_matrix_covid):
    # Select the columns from the matrices
    result = []
    for i in range(1, matrix_size + 1):
        col1_elements = set(Id_matrix_covid[:i])
        col2_elements = set(metrics_matrix[:i, 0].astype(float))
        intersection = col1_elements.intersection(col2_elements)
        similarity_percentage = (len(intersection) / len(col1_elements))
        result.append(similarity_percentage)
    return result

def compare_columns(metrics_matrix, col_idx,matrix_size,Id_matrix_covid):
    # Select the columns from the matrices
    result = []
    aux = metrics_matrix[metrics_matrix[:, col_idx].argsort()[::-1]]

    for i in range(1, matrix_size + 1):
        col1_elements = set(Id_matrix_covid[:i])

        col2_elements = set(aux[:i, 0].astype(float))
        intersection = col1_elements.intersection(col2_elements)
        similarity_percentage = (len(intersection) / len(col1_elements))

        result.append(similarity_percentage)
    return result

def calculate_random_metric_averages(data_matrix):

    Mselected_data = data_matrix.copy()
    # Extraia a segunda coluna (coluna 'geocode')
    # Embaralhe a lista de geocodes
    for _ in range(10):
        np.random.shuffle(Mselected_data)
    # Atualize a segunda coluna com os geocodes embaralhados
    similarity = compare_random(Mselected_data)
    result_data = {
        "Similarity": similarity}
    result_date_df = pd.DataFrame(result_data)
    result_date_df['id'] = range(1, len(result_date_df) + 1)
    return result_date_df

def graph_plot_DateXMetrics(step,cases,leng,df,matrix_covid_dates):
    rc('text', usetex=True)
    font = {'family' : 'normal',
            'weight' : 'bold',
            'size'   : 12}

    rc('font', **font)
    params = {'legend.fontsize': 12}
    plt.rcParams.update(params)
    plt.rcParams.update({
        "text.usetex": True,
        "font.family": "sans-serif",
        "font.sans-serif":"Helvetica",
    })
    degrees_similarity = df['degrees_similarity'].values
    betweenness_similarity = df['betweenness_similarity'].values
    clustering_similarity = df['clustering_similarity'].values
    strength_similarity = df['strength_similarity'].values
    closeness_w_similarity = df['closeness_w_similarity'].values
    eignv_w_similarity = df['eignv_w_similarity'].values

    # Indices for points along the x-axis intervals
    point_indices = np.linspace(0, len(matrix_covid_dates)-1, num=100)
    date_indices = np.arange(len(matrix_covid_dates))
    # Interpolation of similarity points
    print("Length of matrix_covid_dates:", len(matrix_covid_dates))
    print("Length of degrees_similarity:", len(degrees_similarity))
    degrees_interp = np.interp(point_indices, date_indices, degrees_similarity)
    betweenness_interp = np.interp(point_indices, date_indices, betweenness_similarity)
    clustering_interp = np.interp(point_indices, date_indices, clustering_similarity)
    strength_interp = np.interp(point_indices, date_indices, strength_similarity)
    closeness_w_interp = np.interp(point_indices, date_indices, closeness_w_similarity)
    eignv_w_interp = np.interp(point_indices, date_indices, eignv_w_similarity)
    # Store the interpolation data in a pandas DataFrame
    interp_data_df = pd.DataFrame({
    'Degree': degrees_interp,
    'Weighted Betweenness': betweenness_interp,
    'Clustering': clustering_interp,
    'Weighted Strength': strength_interp,
    'Weighted Closeness': closeness_w_interp,
    'Weighted Eigenvector': eignv_w_interp })

    # Configure the plot

    mean_degrees = np.mean(degrees_interp)
    std_degrees = np.std(degrees_interp)
    print(degrees_interp)
    mean_betweenness = np.mean(betweenness_interp)
    std_betweenness = np.std(betweenness_interp)

    mean_clustering = np.mean(clustering_interp)
    std_clustering = np.std(clustering_interp)

    mean_strength = np.mean(strength_interp)
    std_strength = np.std(strength_interp)

    mean_closeness = np.mean(closeness_w_interp)
    std_closeness = np.std(closeness_w_interp)

    mean_eignv = np.mean(eignv_w_interp)
    std_eignv = np.std(eignv_w_interp)

    # Plotar os gráficos
    plt.figure(figsize=(14, 6))
    plt.plot(point_indices, degrees_interp, label=f'Degree (Mean={mean_degrees:.3f}, Std={std_degrees:.3f})', marker='$o$',color ='SlateBlue')
    plt.plot(point_indices, betweenness_interp, label=f'Weighted Betweenness (Mean={mean_betweenness:.3f}, Std={std_betweenness:.3f})', marker='$x$',color ='DarkSlateBlue')
    plt.plot(point_indices, clustering_interp, label=f'Clustering (Mean={mean_clustering:.3f}, Std={std_clustering:.3f})', marker='$s$',color ='RebeccaPurple')
    plt.plot(point_indices, strength_interp, label=f'Strength (Mean={mean_strength:.3f}, Std={std_strength:.3f})', marker='^',color ='MediumOrchid')
    plt.plot(point_indices, closeness_w_interp, label=f'Weighted Closeness (Mean={mean_closeness:.3f}, Std={std_closeness:.3f})', marker='>',color ='SteelBlue')
    plt.plot(point_indices, eignv_w_interp, label=f'Weighted Eigenvector (Mean={mean_eignv:.3f}, Std={std_eignv:.3f})', marker='+',color ='DarkTurquoise')


    plt.text(0.5, 0.93, f'Number of Cities: {leng}', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
    # Adjust the display of the x-axis
    visible_indices = np.arange(0, len(matrix_covid_dates), step=step)
    visible_dates = matrix_covid_dates[visible_indices]
    plt.tight_layout()
    plt.xticks(visible_indices, visible_dates, rotation=60)
    # Configure the plot title and axis labels
    plt.autoscale(axis='x', tight=True)
    plt.autoscale(axis='y', tight=True)
    plt.xlabel('Date')
    plt.ylabel('Intersection rate')
    # Display the legend
    plt.legend()

    plt.savefig(f'/media/work/romulorocha/IC/Collection_metricsXdate_{cases}cases_RANDNET.pdf')
    # Display the plot
    plt.show()
    plt.close()



def graph_plot_minimum_casesXMedia():
    # # List of plot styles for different measures
    # shapes = ['ro-', 'go-', 'yo-', 'mo-', 'co-', 'ko-']

    # # Set up the plot figure
    # plt.figure(figsize=(10, 6))
    # # Labels for the measures
    # labels = ['Degrees', 'Betweenness', 'Clustering', 'Strength', ' Weighted Closeness ', ' Weighted Eigenvector ']
    # # Data for each measure
    # data = [degrees_avg, betweenness_avg, clustering_avg, strength_avg, closeness_w_avg, eignv_w_avg]
    # # Ensure min_cases has the same length as each values array
    # min_cases_expanded = [min_cases] * len(data)
    # # Iterate over measures, plot the data, and add labels
    # for label, values, shape, min_cases_values in zip(labels, data, shapes, min_cases_expanded):
    #     plt.plot(min_cases_values, values, shape, label=f'Avg: {round(sum(values) / len(values), 2)} - {label}')
    #     # Add text annotations for each point
    #     for x, y in zip(min_cases_values, values):
    #         plt.text(x, y, str(round(y, 2)), ha='center', va='bottom')
    # # Set axis labels
    # plt.xlabel('Minimum number of cases')
    # plt.ylabel('Average Similarity')
    # # Automatically adjust the plot to remove empty space after data
    # plt.autoscale(axis='x', tight=True)
    # # Add legend
    # plt.legend(title='Average Similarity', loc='best')
    # # Adjust layout
    # plt.tight_layout()
    # # Save the plot as an image file
    # plt.savefig('Datas/results/graph_minimum-Media.pdf')
    # # Close the plot to release resources
    # plt.close()


    plt.figure(figsize=(10, 6))
    labels = ['Degrees', 'Weighted Betweenness', 'Clustering', 'Weighted Strength', ' Weighted Closeness ', ' Weighted Eigenvector', 'R-Mean', 'R-std']
    data = [degrees_integral, betweenness_integral, clustering_integral, strength_integral, closeness_w_integral, eignv_w_integral,Random_mean_integral,Random_std_integral]
    with open('/media/work/romulorocha/IC/Datas/results/table_Integral_Collection_Comparison.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Minimum number of cases'] + labels)

        for min_case, values in zip(min_cases, zip(*data)):
            writer.writerow([min_case] + list(values))
    print("FEito")

# graph_name = "IC/Datas/networks/grafo_Peso_Geral.GraphML"
# list_cases_name = "IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
# min_cases = [1]
# matrix_size = 0
# df = SetComparison(graph_name,list_cases_name,min_cases)

# graph_plot_DateXMetrics(255,min_cases[0],matrix_size,df)

"""**Codigo de novas correlações**"""

# import scipy.stats as stats

# df_corr = pd.DataFrame({
#     'Original set': covid_cases_og,
#     'Betweenness': betweenness_similarity,
#     'Degree': degrees_similarity,
#     'Closeness': closeness_w_similarity,
#     'Clustering': clustering_similarity,
#     'Eigenvector': eignv_w_similarity,
#     'Strength': strength_similarity,
# })


# # Calculando as correlações de Pearson, Kendall e Spearman
# pearson_corr = df_corr.corr(method='pearson')
# kendall_corr = df_corr.corr(method='kendall')
# spearman_corr = df_corr.corr(method='spearman')

# pearson_pvalue = df_corr.corr(method=lambda x, y: stats.pearsonr(x, y)[1])
# kendall_pvalue = df_corr.corr(method=lambda x, y: stats.kendalltau(x, y)[1])
# spearman_pvalue = df_corr.corr(method=lambda x, y: stats.spearmanr(x, y)[1])

# mask = np.triu(np.ones_like(pearson_corr, dtype=bool))

# # Plotando os heatmaps
# fig, axes = plt.subplots(2, 3, figsize=(15, 10), dpi=300)

# sns.heatmap(pearson_corr, ax=axes[0, 0], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[0, 0].set_title("Pearson Correlation")

# sns.heatmap(kendall_corr, ax=axes[0, 1], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[0, 1].set_title("Kendall Correlation")

# sns.heatmap(spearman_corr, ax=axes[0, 2], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[0, 2].set_title("Spearman Correlation")

# sns.heatmap(pearson_pvalue, ax=axes[1, 0], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[1, 0].set_title("Pearson p-value")

# sns.heatmap(kendall_pvalue, ax=axes[1, 1], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[1, 1].set_title("Kendall p-value")

# sns.heatmap(spearman_pvalue, ax=axes[1, 2], annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5, mask=mask)
# axes[1, 2].set_title("Spearman p-value")

# plt.xticks(rotation=0)
# plt.tight_layout()
# plt.savefig(f'IC/CorrelationsPvalues.pdf', bbox_inches='tight')
# plt.show()

"""# Random network

"""



# coding: utf-8
# Importando bibliotecas necessárias
import os
import numpy as np
import polars as pl
import networkx as nx
import concurrent.futures




def construindo_rede_mobilidade(df_ibge, codigos_municipios):
    """
    Constrói a rede de mobilidade a partir dos dados do IBGE.

    Parâmetros:
    df_ibge (pl.DataFrame): DataFrame contendo os dados do IBGE.
    codigos_municipios (array): Array contendo os códigos dos municípios.

    Retorna:
    tuple: Contendo edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo.
    """
    # Dicionários para mapeamento de nomes e códigos dos municípios
    nome_municipios = {}
    codigo_para_indice = {}
    indice_para_codigo = {}

    # Preenchendo os dicionários com códigos e nomes dos municípios
    for i, codigo in enumerate(codigos_municipios):
        # Filtrando as linhas correspondentes ao código nas duas colunas
        linhas_a = df_ibge.filter(df_ibge['CODMUNDV_A'] == codigo)
        linhas_b = df_ibge.filter(df_ibge['CODMUNDV_B'] == codigo)

        # Selecionando o nome do município
        nome_municipio = linhas_a['NOMEMUN_A'][0] if not linhas_a.is_empty() else linhas_b['NOMEMUN_B'][0]

        # Adicionando ao dicionário
        nome_municipios[i] = nome_municipio
        codigo_para_indice[int(codigo)] = i
        indice_para_codigo[i] = int(codigo)

    # Criando o array de arestas (edges)
    edges = np.zeros((len(df_ibge), 2), dtype=int)

    # Preenchendo o array de arestas com os índices associados aos códigos únicos
    for i, (codigo_a, codigo_b) in enumerate(df_ibge[['CODMUNDV_A', 'CODMUNDV_B']].to_numpy()):
        edges[i, 0] = codigo_para_indice[codigo_a]
        edges[i, 1] = codigo_para_indice[codigo_b]

    # Calcula os pesos para cada aresta
    weights = (df_ibge['VAR05'] + df_ibge['VAR06'] + df_ibge['VAR12']).to_numpy()

    return edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo


def rede_aleatoria_processo_paralelismo(iteracao, edges, weights, indice_para_codigo):
    # Extrair e armazenar o vetor de graus da rede original
    grafo_original = nx.Graph()
    grafo_original.add_edges_from(edges)
    graus_originais = [grafo_original.degree(n) for n in grafo_original.nodes()]

    # Lista de pesos já extraída
    pesos_originais = weights

    # Lista de pares nome/geocódigo
    pares_nome_geocodigo = list(indice_para_codigo.items())

    # Embaralhar o vetor de graus
    graus_embaralhados = np.random.permutation(graus_originais)

    # Gerar a rede aleatorizada via Configuration Model
    rede_aleatorizada = nx.configuration_model(graus_embaralhados)
    rede_aleatorizada = nx.Graph(rede_aleatorizada)  # Remove self-loops e parallel edges

    # Atribuir pesos aleatórios a partir da lista de pesos originais
    pesos_aleatorios = np.random.choice(pesos_originais, size=rede_aleatorizada.number_of_edges())

    # Atribuir nomes/geocodes aos nós aleatoriamente
    np.random.shuffle(pares_nome_geocodigo)
    nome_geocode_dict = {id: geocode for id, geocode in pares_nome_geocodigo}

    # Atribuir os nomes e pesos aos nós e arestas
    mapping = {node: nome_geocode_dict[node] for node in rede_aleatorizada.nodes()}
    rede_aleatorizada = nx.relabel_nodes(rede_aleatorizada, mapping)

    # Adicionar atributo geocode aos nós
    for i, node in enumerate(rede_aleatorizada.nodes()):
        rede_aleatorizada.nodes[node]['geocode'] = nome_geocode_dict[i]

    # Atribuir pesos às arestas
    for j, (u, v) in enumerate(rede_aleatorizada.edges()):
        rede_aleatorizada.edges[u, v]['weight'] = pesos_aleatorios[j]

    # Salvar a rede aleatorizada em um arquivo GraphML
    graph_name = f'/media/work/romulorocha/IC/Datas/networks/rede_aleatorizada_{iteracao+1}.graphml'
    nx.write_graphml(rede_aleatorizada, graph_name)

    # Aqui você pode adicionar o código para rodar os Algoritmos 1 e 2 e coletar os resultados
    list_cases_name = "/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
    min_cases = [5]
    df_acc, Cumulative_Cases_Day = Acummulate(graph_name, list_cases_name, min_cases)
    df, matrix_covid_dates = SetComparison(graph_name, list_cases_name, min_cases)

    print(f"Iteração {iteracao + 1} completa.")
    
    return df, df_acc, matrix_covid_dates, Cumulative_Cases_Day

def gerar_rede_aleatorizada(edges, weights, indice_para_codigo, num_iteracoes=2, max_workers=None):
    # Armazenar os DataFrames em listas para posterior análise
    dataframes_stc_list = []
    dataframes_acc_list = []
    matrix_covid_dates_list = []
    cumulative_cases_day_list = []

    # Usar ThreadPoolExecutor para paralelizar as iterações
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(rede_aleatoria_processo_paralelismo, iteracao, edges, weights, indice_para_codigo) for iteracao in range(num_iteracoes)]
        
        for future in concurrent.futures.as_completed(futures):
            df, df_acc, matrix_covid_dates, cumulative_cases_day = future.result()
            dataframes_stc_list.append(df)
            dataframes_acc_list.append(df_acc)
            matrix_covid_dates_list.append(matrix_covid_dates)
            cumulative_cases_day_list.append(cumulative_cases_day)

    print(dataframes_acc_list)
    # Retorne os resultados desejados
    return dataframes_stc_list, dataframes_acc_list, matrix_covid_dates_list, cumulative_cases_day_list


# Define o caminho para o arquivo do DataFrame do IBGE de 2016 na pasta 'raw_data'
caminho_df_ibge = '/media/work/romulorocha/IC/Datas/Pre-processed/dataset_transform_IBGE.xlsx'

# Carregando o dataset IBGE de ligações entre cidades com pesos
df_ibge = pl.read_excel(caminho_df_ibge)

# Obtendo códigos únicos de ambas as colunas
codigos_municipios = np.unique(
    np.concatenate([df_ibge['CODMUNDV_A'].to_numpy(), df_ibge['CODMUNDV_B'].to_numpy()]))

# Construindo a rede de mobilidade
edges, weights, nome_municipios, codigo_para_indice, indice_para_codigo = (
    construindo_rede_mobilidade(df_ibge, codigos_municipios))


# Chamando a função para gerar a rede aleatorizada
dataframes_stc_list, dataframes_acc_list, matrix_covid_dates_list, cumulative_cases_day_list = gerar_rede_aleatorizada(edges, weights, indice_para_codigo, num_iteracoes=700, max_workers=10)

# Calcular a média de casos aleatorios
concatenated_df = pd.concat(dataframes_stc_list, axis=0)
mean_df_stc = concatenated_df.groupby(concatenated_df.index).mean()

concatenated_df = pd.concat(dataframes_acc_list, axis=0)
mean_df_acc = concatenated_df.groupby(concatenated_df.index).mean()

list_cases_name = "/media/work/romulorocha/IC/Datas/Pre-processed/cases-brazil-cities-time_2020.csv"
min_cases = [5]
leng = len(mean_df_stc)
step = 255

file_path_stc = '/media/work/romulorocha/IC/mean_df_stc.csv'
file_path_acc = '/media/work/romulorocha/IC/mean_df_acc.csv'

# Salvar mean_df_stc em um arquivo CSV
mean_df_stc.to_csv(file_path_stc, index=False)
print(f'mean_df_stc salvo no arquivo {file_path_stc}')

# Salvar mean_df_acc em um arquivo CS
mean_df_acc.to_csv(file_path_acc, index=False)
print(f'mean_df_acc salvo no arquivo {file_path_acc}')

# Salvar matrix_covid_dates_list em um arquivo CSV
file_path_matrix_covid_dates = '/media/work/romulorocha/IC/matrix_covid_dates_list.csv'
concatenated_matrix_covid_dates = pd.concat(matrix_covid_dates_list, axis=0)
concatenated_matrix_covid_dates.to_csv(file_path_matrix_covid_dates, index=False)
print(f'matrix_covid_dates_list salvo no arquivo {file_path_matrix_covid_dates}')

# Salvar cumulative_cases_day_list em um arquivo CSV
file_path_cumulative_cases_day = '/media/work/romulorocha/IC/cumulative_cases_day_list.csv'
concatenated_cumulative_cases_day = pd.concat(cumulative_cases_day_list, axis=0)
concatenated_cumulative_cases_day.to_csv(file_path_cumulative_cases_day, index=False)
print(f'cumulative_cases_day_list salvo no arquivo {file_path_cumulative_cases_day}')

